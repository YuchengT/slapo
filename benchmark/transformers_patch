diff --git a/src/transformers/trainer.py b/src/transformers/trainer.py
index c1869ef76..06d33d24b 100755
--- a/src/transformers/trainer.py
+++ b/src/transformers/trainer.py
@@ -1712,7 +1712,9 @@ class Trainer:
                 self._load_rng_state(resume_from_checkpoint)

             step = -1
+            step_metrics = {"step_time_list": []}
             for step, inputs in enumerate(epoch_iterator):
+                step_start = time.time()

                 # Skip past any already trained steps if resuming training
                 if steps_trained_in_current_epoch > 0:
@@ -1819,6 +1821,7 @@ class Trainer:
                 else:
                     self.control = self.callback_handler.on_substep_end(args, self.state, self.control)

+                step_metrics["step_time_list"].append(time.time() - step_start)
                 if self.control.should_epoch_stop or self.control.should_training_stop:
                     break
             if step < 0:
@@ -1865,6 +1868,7 @@ class Trainer:
         train_loss = self._total_loss_scalar / self.state.global_step

         metrics = speed_metrics("train", start_time, num_samples=num_train_samples, num_steps=self.state.max_steps)
+        metrics.update(step_metrics)
         self.store_flos()
         metrics["total_flos"] = self.state.total_flos
         metrics["train_loss"] = train_loss
diff --git a/src/transformers/trainer_pt_utils.py b/src/transformers/trainer_pt_utils.py
index 7ff0eb51a..ea5134e5a 100644
--- a/src/transformers/trainer_pt_utils.py
+++ b/src/transformers/trainer_pt_utils.py
@@ -867,6 +867,8 @@ def metrics_format(self, metrics: Dict[str, float]) -> Dict[str, float]:
             metrics_copy[k] = f"{ int(v) >> 30 }GF"
         elif type(metrics_copy[k]) == float:
             metrics_copy[k] = round(v, 4)
+        elif "_list" in k:
+            metrics_copy[k] = f"{','.join(str(round(e, 4)) for e in v)}"

     return metrics_copy
