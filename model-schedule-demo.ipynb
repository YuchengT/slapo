{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8b99b44",
   "metadata": {},
   "source": [
    "# Decouple Model Execution from Definition\n",
    "\n",
    "Hongzheng Chen, Cody Hao Yu, Shuai Zheng\n",
    "\n",
    "\n",
    "## Background and Motivation\n",
    "* **Performance**: Large gap between native implementations and optimized models. From Mu's slide\n",
    "    | Model\\Performance (TFLOPs) | HuggingFace | Megatron-LM |\n",
    "    | :--: | :--: | :--: |\n",
    "    | BERT | 31 | **43** |\n",
    "    | GPT2 | 19 | **42** |\n",
    "* **Productivity**\n",
    "    * Megatron-LM, DeepSpeed ZeRO-3: Parameter sharding; MiCS: prefetching, buffer pre-allocation\n",
    "    * Manually modify the model\n",
    "    * Components are not reusable for models other than Transformers\n",
    "* **Customizability**\n",
    "    * Alpa automatically searches for the optimial 3D parallelism\n",
    "    * Compilation passes are monolithic: Cannot just do some optimizations for specific layers and see results (e.g. shard an op)\n",
    "    * Optimization are opaque: Hard to locate the issues in the compiler\n",
    "\n",
    "\n",
    "## Proposal: A Model Scheduling DSL\n",
    "Decouple model execution from definition\n",
    "* TVM/Halide: Only consider op-level optimization and only for single machine inference workload\n",
    "\n",
    "Cover optimizations: (which requires manually changing the models in existing works)\n",
    "1. Parameter sharding\n",
    "2. Kernel fusion/injection\n",
    "3. Gradient checkpointing\n",
    "4. Memory defragmentation\n",
    "5. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acf2401",
   "metadata": {},
   "source": [
    "## Demos\n",
    "\n",
    "### (1) Kernel Injection\n",
    "\n",
    "Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2314589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, copy, time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ms # model-scheduling\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e6b7a8",
   "metadata": {},
   "source": [
    "Create a simple MLP model with layer norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54d1b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2048\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.dense_1 = nn.Linear(dim, dim * 2)\n",
    "        self.layer_norm = nn.LayerNorm([dim, dim * 2])\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dense_2 = nn.Linear(dim * 2, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense_1(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dense_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4982ef55",
   "metadata": {},
   "source": [
    "Instanciate the model and create an optimizer for traing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d28207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "model = MLP(N).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a099060",
   "metadata": {},
   "source": [
    "Create a default schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3858651",
   "metadata": {},
   "outputs": [],
   "source": [
    "sch = ms.create_schedule(copy.deepcopy(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a7b909",
   "metadata": {},
   "source": [
    "Currently we use torch.fx to trace the model and generate IR for optimization. We can print out the graph module to see the operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff71924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sch.gm.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0859ba",
   "metadata": {},
   "source": [
    "Print operators in the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbae77fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ops = sch.forward_ops\n",
    "print(ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94600d77",
   "metadata": {},
   "source": [
    "Replace layer_norm with Apex layer_norm. Just a single line!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e38892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apex.normalization.fused_layer_norm import FusedLayerNorm\n",
    "\n",
    "sch[ops[1]].replace(FusedLayerNorm, [N, N * 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e71e9d",
   "metadata": {},
   "source": [
    "Apply the schedule and regenerate the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7739c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_model, optimizer = ms.build(sch)\n",
    "print(opt_model.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3129b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.rand(N, N).to(device)\n",
    "original_output = model(inp)\n",
    "optimized_output = opt_model(inp)\n",
    "np.testing.assert_almost_equal(original_output.cpu().detach().numpy(), optimized_output.cpu().detach().numpy(), decimal=5)\n",
    "print(\"Results are correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8160bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_time = []\n",
    "apex_time = []\n",
    "for i in range(100):\n",
    "    inp = torch.rand(N, N).to(device)\n",
    "    # Test native PyTorch implementation\n",
    "    start_time = time.time()\n",
    "    original_output = model(inp)\n",
    "    pt_time.append((time.time() - start_time) * 1000)\n",
    "    # Test Apex function\n",
    "    start_time = time.time()\n",
    "    optimized_output = opt_model(inp)\n",
    "    apex_time.append((time.time() - start_time) * 1000)\n",
    "\n",
    "# plot results\n",
    "plt.plot(np.arange(100), pt_time, label=\"pytorch\")\n",
    "plt.plot(np.arange(100), apex_time, label=\"apex\")\n",
    "plt.legend()\n",
    "print(\"Pytorch: {:.4f}ms\".format(np.mean(pt_time)))\n",
    "print(\"Apex: {:.4f}ms\".format(np.mean(apex_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b187e3be",
   "metadata": {},
   "source": [
    "### (1.1) Kernel Fusion\n",
    "\n",
    "Similarly, we can replace a series of ops with a single fused op/block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a69368",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusedBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(dim, dim * 2)\n",
    "        self.ln = nn.LayerNorm([dim, dim * 2])\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.ln(self.fc(x)))\n",
    "        return x\n",
    "\n",
    "sch = ms.create_schedule(copy.deepcopy(model))\n",
    "ops = sch.forward_ops\n",
    "sch[ops[0:3]].replace(FusedBlock, N)\n",
    "print(sch.gm.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08e54c8",
   "metadata": {},
   "source": [
    "### (2) Parameter Sharding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66856972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rank, world_size):\n",
    "    print(f\"Running basic MLP example on rank {rank}.\")\n",
    "\n",
    "    # === Model execution schedule ===\n",
    "    model = MLP(32).cuda(rank)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.002)\n",
    "\n",
    "    # Create a default schedule\n",
    "    sch = ms.create_schedule(model, optimizer, world_size, rank)\n",
    "    \n",
    "    # Access operators\n",
    "    ops = sch.forward_ops\n",
    "\n",
    "    # Partition parameters\n",
    "    # column sharding for dense_1\n",
    "    sch[ops[0]].partition(axis=0, param=\"weight\")\n",
    "    # row sharding for dense_2\n",
    "    sch[ops[3]].partition(axis=1, param=\"weight\")\n",
    "\n",
    "    # Partition outputs\n",
    "    # The result from dense_2 needs aggregation by dim 0\n",
    "    sch[ops[3]].partition(axis=0)\n",
    "\n",
    "    # Apply schedule and regenerate module\n",
    "    model, optimizer = ms.build(sch)\n",
    "\n",
    "    # Perform a num of iterations of forward/backward\n",
    "    # and optimizations for the sharded module.\n",
    "    for i in range(5):\n",
    "        start_time = time.time()\n",
    "        inp = torch.rand(16, 32).cuda(rank)\n",
    "        output = model(inp)\n",
    "        output.sum().backward()\n",
    "        optimizer.step()\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Finish step {i}, time: {elapsed_time:.10f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e4425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59d022d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
